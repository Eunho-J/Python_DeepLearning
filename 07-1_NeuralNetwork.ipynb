{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780f7bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6017404",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4be75c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#유틸 함수들\n",
    "def sigmoid(x):\n",
    "    return 1/ (1 + np.exp(-x))\n",
    "\n",
    "def mean_squared_error(pred_y, true_y):\n",
    "    return np.mean(np.sum(np.square((true_y - pred_y))))\n",
    "\n",
    "def cross_entropy_error(pred_y, true_y):\n",
    "    if true_y.ndim == 1:\n",
    "        true_y = true_y.reshape(1, -1)\n",
    "        pred_y = pred_y.reshape(1, -1)\n",
    "        \n",
    "    delta = 1e-7\n",
    "    return -np.sum(true_y * np.log(pred_y + delta))\n",
    "\n",
    "def cross_entropy_error_for_batch(pred_y, true_y):\n",
    "    if true_y.ndim == 1:\n",
    "        true_y = true_y.reshape(1, -1)\n",
    "        pred_y = pred_y.reshape(1, -1)\n",
    "        \n",
    "    delta = 1e-7\n",
    "    batch_size = pred_y.shape[0]\n",
    "    return -np.sum(true_y * np.log(pred_y + delta)) / batch_size\n",
    "\n",
    "def cross_entropy_error_for_bin(pred_y, true_y):\n",
    "    return 0.5 * np.sum((-true_y * np.log(pred_y) - (1 - true_y) * np.log(1 - pred_y)))\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    return exp_a / sum_exp_a\n",
    "\n",
    "def differential(f, x):\n",
    "    eps = 1e-5\n",
    "    diff_value = np.zeros_like(x)\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        temp_val = x[i]\n",
    "        \n",
    "        x[i] = temp_val + eps\n",
    "        f_h1 = f(x)\n",
    "        \n",
    "        x[i] = temp_val - eps\n",
    "        f_h2 = f(x)\n",
    "        \n",
    "        diff_value[i] = (f_h1 - f_h2) / (2 * eps)\n",
    "        x[i] = temp_val\n",
    "        \n",
    "    return diff_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18171bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#신경망\n",
    "class LogicGateNet():\n",
    "    \n",
    "    def __init__(self):\n",
    "        def weight_init():\n",
    "            np.random.seed(1)\n",
    "            weights = np.random.randn(2)\n",
    "            bias = np.random.rand(1)\n",
    "            \n",
    "            return weights, bias\n",
    "        \n",
    "        self.weights, self.bias = weight_init()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W = self.weights.reshape(-1, 1)\n",
    "        b = self.bias\n",
    "        \n",
    "        pred_y = sigmoid(np.dot(x, W) + b)\n",
    "        return pred_y\n",
    "    \n",
    "    def loss(self, x, true_y):\n",
    "        pred_y = self.predict(x)\n",
    "        return cross_entropy_error_for_bin(pred_y, true_y)\n",
    "    \n",
    "    def get_gradient(self, x, t):\n",
    "        def loss_grad(grad):\n",
    "            return self.loss(x, t)\n",
    "        \n",
    "        grad_W = differential(loss_grad, self.weights)\n",
    "        grad_b = differential(loss_grad, self.bias)\n",
    "        \n",
    "        return grad_W, grad_b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "972c6f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Cost: 0.6886489498071491, Weights: [1.56426876 0.79168393], Bias: [-2.14871589]\n",
      "Epoch: 200, Cost: 0.4946368603064415, Weights: [2.01360719 1.71241131], Bias: [-3.07894028]\n",
      "Epoch: 300, Cost: 0.3920165980757418, Weights: [2.42841657 2.29753793], Bias: [-3.79103207]\n",
      "Epoch: 400, Cost: 0.3257214374791936, Weights: [2.794852   2.73235738], Bias: [-4.37257095]\n",
      "Epoch: 500, Cost: 0.27863601334755067, Weights: [3.11636193 3.08408364], Bias: [-4.86571237]\n",
      "Epoch: 600, Cost: 0.24328504683831248, Weights: [3.40015395 3.38235762], Bias: [-5.29433736]\n",
      "Epoch: 700, Cost: 0.21572536552468008, Weights: [3.65300561 3.64264217], Bias: [-5.67349792]\n",
      "Epoch: 800, Cost: 0.19363244428365756, Weights: [3.88044124 3.87412053], Bias: [-6.01340133]\n",
      "Epoch: 900, Cost: 0.1755321312790001, Weights: [4.08680123 4.08279091], Bias: [-6.32133891]\n",
      "Epoch: 1000, Cost: 0.1604392693330146, Weights: [4.27548114 4.27284863], Bias: [-6.6027234]\n"
     ]
    }
   ],
   "source": [
    "#AND Gate\n",
    "AND = LogicGateNet()\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "train_loss_list = list()\n",
    "\n",
    "for i in range(epochs):\n",
    "    grad_W, grad_b = AND.get_gradient(X, Y)\n",
    "    \n",
    "    AND.weights -= lr * grad_W\n",
    "    AND.bias -= lr * grad_b\n",
    "    \n",
    "    loss = AND.loss(X, Y)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print(\"Epoch: {}, Cost: {}, Weights: {}, Bias: {}\".format(i+1, loss, AND.weights, AND.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12b2c4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00135483]\n",
      " [0.08867878]\n",
      " [0.08889176]\n",
      " [0.87496677]]\n"
     ]
    }
   ],
   "source": [
    "print(AND.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2853201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Cost: 0.49580923848195635, Weights: [2.45484353 1.40566594], Bias: [-0.14439625]\n",
      "Epoch: 200, Cost: 0.3398674231515118, Weights: [2.98631846 2.39448393], Bias: [-0.67661178]\n",
      "Epoch: 300, Cost: 0.2573360986187996, Weights: [3.45016595 3.08431266], Bias: [-1.03721585]\n",
      "Epoch: 400, Cost: 0.20630142190075948, Weights: [3.85230067 3.60865952], Bias: [-1.30598633]\n",
      "Epoch: 500, Cost: 0.1716549922113493, Weights: [4.20195872 4.03000824], Bias: [-1.52060015]\n",
      "Epoch: 600, Cost: 0.1466501884550824, Weights: [4.50867681 4.38171478], Bias: [-1.6994397]\n",
      "Epoch: 700, Cost: 0.12779768649454676, Weights: [4.78049264 4.68334611], Bias: [-1.8527641]\n",
      "Epoch: 800, Cost: 0.11310517185413338, Weights: [5.0237707 4.9472786], Bias: [-1.98691756]\n",
      "Epoch: 900, Cost: 0.10135180918376233, Weights: [5.24347159 5.18181684], Bias: [-2.10611973]\n",
      "Epoch: 1000, Cost: 0.09174843008614178, Weights: [5.44346811 5.39279833], Bias: [-2.21332947]\n"
     ]
    }
   ],
   "source": [
    "OR = LogicGateNet()\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "train_loss_list = list()\n",
    "\n",
    "for i in range(epochs):\n",
    "    grad_W, grad_b = OR.get_gradient(X, Y)\n",
    "    \n",
    "    OR.weights -= lr * grad_W\n",
    "    OR.bias -= lr * grad_b\n",
    "    \n",
    "    loss = OR.loss(X, Y)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print(\"Epoch: {}, Cost: {}, Weights: {}, Bias: {}\".format(i+1, loss, OR.weights, OR.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f486bc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09855987]\n",
      " [0.9600543 ]\n",
      " [0.96195283]\n",
      " [0.9998201 ]]\n"
     ]
    }
   ],
   "source": [
    "print(OR.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da2d96c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Cost: 0.7911738653769252, Weights: [-0.48972722 -1.25798774], Bias: [1.74566135]\n",
      "Epoch: 200, Cost: 0.5430490957885361, Weights: [-1.51545093 -1.80261804], Bias: [2.79151756]\n",
      "Epoch: 300, Cost: 0.4212591302740578, Weights: [-2.14614496 -2.26642639], Bias: [3.56506179]\n",
      "Epoch: 400, Cost: 0.3456117101527486, Weights: [-2.607325   -2.66303355], Bias: [4.18521187]\n",
      "Epoch: 500, Cost: 0.2931298605179329, Weights: [-2.97696333 -3.00501941], Bias: [4.70528682]\n",
      "Epoch: 600, Cost: 0.2543396786002071, Weights: [-3.28850585 -3.30365261], Bias: [5.1539571]\n",
      "Epoch: 700, Cost: 0.22443918596775067, Weights: [-3.55912171 -3.56778782], Bias: [5.54869527]\n",
      "Epoch: 800, Cost: 0.20067626330853877, Weights: [-3.7989077  -3.80411461], Bias: [5.90108417]\n",
      "Epoch: 900, Cost: 0.18134125517637367, Weights: [-4.01441395 -4.01767547], Bias: [6.21926514]\n",
      "Epoch: 1000, Cost: 0.1653094408173465, Weights: [-4.21019696 -4.21231432], Bias: [6.50920952]\n"
     ]
    }
   ],
   "source": [
    "NAND = LogicGateNet()\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[1], [1], [1], [0]])\n",
    "\n",
    "train_loss_list = list()\n",
    "\n",
    "for i in range(epochs):\n",
    "    grad_W, grad_b = NAND.get_gradient(X, Y)\n",
    "    \n",
    "    NAND.weights -= lr * grad_W\n",
    "    NAND.bias -= lr * grad_b\n",
    "    \n",
    "    loss = NAND.loss(X, Y)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print(\"Epoch: {}, Cost: {}, Weights: {}, Bias: {}\".format(i+1, loss, NAND.weights, NAND.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa03f7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99851256]\n",
      " [0.90861957]\n",
      " [0.90879523]\n",
      " [0.12861037]]\n"
     ]
    }
   ],
   "source": [
    "print(NAND.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b84db60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Cost: 1.4026852245456056, Weights: [ 0.47012771 -0.19931523], Bias: [-0.16097708]\n",
      "Epoch: 200, Cost: 1.3879445622848308, Weights: [ 0.1572739  -0.03387161], Bias: [-0.07321056]\n",
      "Epoch: 300, Cost: 1.386492030048381, Weights: [0.05525161 0.00089673], Bias: [-0.03330094]\n",
      "Epoch: 400, Cost: 1.3863236205351948, Weights: [0.02049628 0.00504503], Bias: [-0.01514784]\n",
      "Epoch: 500, Cost: 1.3862994743646844, Weights: [0.0080051  0.00361297], Bias: [-0.00689034]\n",
      "Epoch: 600, Cost: 1.3862953430687464, Weights: [0.00326661 0.00201812], Bias: [-0.00313421]\n",
      "Epoch: 700, Cost: 1.3862945581495083, Weights: [0.00137938 0.00102449], Bias: [-0.00142566]\n",
      "Epoch: 800, Cost: 1.38629440139037, Weights: [0.00059716 0.00049628], Bias: [-0.00064849]\n",
      "Epoch: 900, Cost: 1.3862943694120307, Weights: [0.00026303 0.00023435], Bias: [-0.00029498]\n",
      "Epoch: 1000, Cost: 1.386294362832352, Weights: [0.0001172  0.00010905], Bias: [-0.00013418]\n"
     ]
    }
   ],
   "source": [
    "#단층 신경망으로는 구현이 불가능한 케이스 존재\n",
    "XOR = LogicGateNet()\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "train_loss_list = list()\n",
    "\n",
    "for i in range(epochs):\n",
    "    grad_W, grad_b = XOR.get_gradient(X, Y)\n",
    "    \n",
    "    XOR.weights -= lr * grad_W\n",
    "    XOR.bias -= lr * grad_b\n",
    "    \n",
    "    loss = XOR.loss(X, Y)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print(\"Epoch: {}, Cost: {}, Weights: {}, Bias: {}\".format(i+1, loss, XOR.weights, XOR.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b250b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49996646]\n",
      " [0.49999372]\n",
      " [0.49999575]\n",
      " [0.50002302]]\n"
     ]
    }
   ],
   "source": [
    "print(XOR.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fca28221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2층 신경망으로 XOR 게이트 구현\n",
    "class XORNet():\n",
    "    \n",
    "    def __init__(self):\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        def weight_init():\n",
    "            params = {}\n",
    "            params['w_1'] = np.random.randn(2)\n",
    "            params['b_1'] = np.random.rand(2)\n",
    "            params['w_2'] = np.random.randn(2)\n",
    "            params['b_2'] = np.random.rand(1)\n",
    "            return params\n",
    "        \n",
    "        self.params = weight_init()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W_1, W_2 = self.params['w_1'].reshape(-1,1), self.params['w_2'].reshape(-1,1)\n",
    "        B_1, B_2 = self.params['b_1'], self.params['b_2']\n",
    "        \n",
    "        A1 = np.dot(x, W_1) + B_1\n",
    "        Z1 = sigmoid(A1)\n",
    "        A2 = np.dot(Z1, W_2) + B_2\n",
    "        pred_y = sigmoid(A2)\n",
    "        \n",
    "        return pred_y\n",
    "    \n",
    "    def loss(self, x, true_y):\n",
    "        pred_y = self.predict(x)\n",
    "        return cross_entropy_error_for_bin(pred_y, true_y)\n",
    "    \n",
    "    def get_gradient(self, x, t):\n",
    "        def loss_grad(grad):\n",
    "            return self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['w_1'] = differential(loss_grad, self.params['w_1'])\n",
    "        grads['b_1'] = differential(loss_grad, self.params['b_1'])\n",
    "        grads['w_2'] = differential(loss_grad, self.params['w_2'])\n",
    "        grads['b_2'] = differential(loss_grad, self.params['b_2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4a7fa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, cost: 1.3535614442470036\n",
      "Epoch: 200, cost: 1.2827154568316697\n",
      "Epoch: 300, cost: 0.8968907892186366\n",
      "Epoch: 400, cost: 0.33871971411928997\n",
      "Epoch: 500, cost: 0.18121344476191775\n",
      "Epoch: 600, cost: 0.11991186457358068\n",
      "Epoch: 700, cost: 0.08861936864741338\n",
      "Epoch: 800, cost: 0.06992180653088811\n",
      "Epoch: 900, cost: 0.0575804135303371\n",
      "Epoch: 1000, cost: 0.04886093568413276\n"
     ]
    }
   ],
   "source": [
    "lr = 0.3\n",
    "\n",
    "XOR = XORNet()\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "train_loss_list = list()\n",
    "\n",
    "for i in range(epochs):\n",
    "    grads = XOR.get_gradient(X, Y)\n",
    "    \n",
    "    for key in ('w_1', 'b_1', 'w_2', 'b_2'):\n",
    "        XOR.params[key] -= lr * grads[key]\n",
    "        \n",
    "    loss = XOR.loss(X, Y)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print('Epoch: {}, cost: {}'.format(i+1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "397097f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0217367 ]\n",
      " [0.96884394]\n",
      " [0.97816819]\n",
      " [0.0217794 ]]\n"
     ]
    }
   ],
   "source": [
    "print(XOR.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d84ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
